python3 cli.py \
--method sequence_classifier \
--pattern_ids 0 \
--data_dir data/ \
--model_type roberta \
--model_name_or_path roberta-large \
--task_name mnli \
--output_dir experiments//roberta/supervised \
--do_train \
--do_eval \
--pet_per_gpu_eval_batch_size 2 \
--pet_per_gpu_train_batch_size 2 \
--pet_gradient_accumulation_steps 8 \
--pet_num_train_epochs 5 \
--pet_max_seq_length 256 \
--pet_repetitions 1 \
--sc_per_gpu_train_batch_size 2 \
--sc_per_gpu_unlabeled_batch_size 8 \
--sc_gradient_accumulation_steps 8 \
--sc_num_train_epochs 5 \
--sc_max_seq_length 256 \
--sc_repetitions 1 \
--train_examples 5 10 50 100 \
--overwrite_output_dir \
--no_distillation
